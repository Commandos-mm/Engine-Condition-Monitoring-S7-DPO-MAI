{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZT7WFMQVAsI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Dict, Tuple, Set, Callable\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_JMxY6O3fxe",
        "outputId": "97fa5b6f-977d-43dd-f687-9081017fb312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtEFpFMc4K6J"
      },
      "outputs": [],
      "source": [
        "def grouper(input: Dict[Tuple[str, ...],pd.DataFrame],\n",
        "            keys: Tuple[str, ...]) -> Dict[Tuple[str, ...], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Helper function for recursive grouping dict of pandas DataFrames.\n",
        "    \"\"\"\n",
        "    if not keys:\n",
        "        return input\n",
        "\n",
        "    to_tuple_transformer: Callable = lambda element: element if isinstance(element, tuple) else (element, )\n",
        "\n",
        "    grouped_df: Dict[Tuple[str, ...], pd.DataFrame] = {\n",
        "        (next_key, *to_tuple_transformer(curr_key)): next_df\n",
        "        for curr_key, curr_df in input.items()\n",
        "        for next_key, next_df in curr_df.groupby(keys[-1])\n",
        "    }\n",
        "\n",
        "    return grouper(grouped_df, keys[:-1])\n",
        "\n",
        "\n",
        "def group(input: pd.DataFrame,\n",
        "          keys: Tuple[str, ...] = (\"engine_family\", \"flight_phase\")) -> Dict[Tuple[str, ...], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Mock function for maintenance characteristics prediction.\n",
        "    :param input: input DataFrame of aircraft and engine characteristics.\n",
        "    :param keys: keys that are used for grouping output data.\n",
        "    :return: output groups of DataFrames of predicted maintenance characteristics.\n",
        "    \"\"\"\n",
        "    phase_df: Dict = {k: v for k, v in input.groupby(keys[-1])}\n",
        "\n",
        "    return grouper(phase_df, keys[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoK8KFsi8DZO"
      },
      "outputs": [],
      "source": [
        "def validate(df):\n",
        "    return df.dropna(axis='columns', thresh=20).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTm1xuisVQNG"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('/content/drive/MyDrive/S7/X.csv')\n",
        "y = pd.read_csv('/content/drive/MyDrive/S7/y.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQrY1qijWC1z"
      },
      "outputs": [],
      "source": [
        "splitted_X = group(X, (\"flight_phase\", \"engine_family\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgGFi4kWWKuZ"
      },
      "outputs": [],
      "source": [
        "merged_y = pd.merge(X[[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"]], y, on=[\"engine_id\", \"flight_datetime\", \"flight_phase\"])\n",
        "splitted_y = group(merged_y, (\"flight_phase\", \"engine_family\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAeMlVA4cQ5W"
      },
      "outputs": [],
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df, incols, outcols):\n",
        "        self.df = df\n",
        "        self.incols = incols\n",
        "        self.outcols = outcols\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #print(self.incols)\n",
        "        #print(self.outcols)\n",
        "        for i in self.df.columns:\n",
        "          (self.df[i]-self.df[i].min())/(self.df[i].max()-self.df[i].min())\n",
        "        input = torch.tensor(self.df[self.incols].iloc[index], dtype=torch.float32)\n",
        "        # input.type(dtype=torch.float32)\n",
        "        output = torch.tensor(self.df[self.outcols].iloc[index], dtype=torch.float32)\n",
        "        # output.type(dtype=torch.float32)\n",
        "        return input, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4qp9V5kLZhd"
      },
      "outputs": [],
      "source": [
        "dataloaders = {}\n",
        "for (key, X_i), (_, y_i) in zip(splitted_X.items(), splitted_y.items()):\n",
        "    X_i = validate(X_i)\n",
        "    y_i = validate(y_i)\n",
        "    input_cols = X_i.drop(columns=[\n",
        "        \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "        \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "        \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"]).columns\n",
        "    output_cols = y_i.drop(columns=[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"]).columns\n",
        "\n",
        "    prepaired = pd.merge(X_i, y_i, on=[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"])\n",
        "    prepaired.drop(columns=[\n",
        "        \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "        \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "        \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"], inplace=True)\n",
        "    \n",
        "    train, test = train_test_split(prepaired, test_size=0.2)\n",
        "    train = train.reset_index()\n",
        "    test = test.reset_index()\n",
        "    train = train.drop(columns='index')\n",
        "    test = test.drop(columns='index')\n",
        "    train['engine_position'] = train['engine_position'].astype('float')\n",
        "    test = test['engine_position'].astype('float')\n",
        "    train_dataset = DataFrameDataset(train, input_cols, output_cols)\n",
        "    test_dataset = DataFrameDataset(test, input_cols, output_cols)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=1)\n",
        "    dataloaders[key] = {\"train\": train_dataloader,\n",
        "                        \"train_size\": len(input_cols),\n",
        "                        \"valid\": test_dataloader,\n",
        "                        \"valid_size\": len(output_cols)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdCjPQ5Bqx3L"
      },
      "outputs": [],
      "source": [
        "dataloaders = {}\n",
        "(key, X_i), (_, y_i) = next(zip(splitted_X.items(), splitted_y.items()))\n",
        "X_i = validate(X_i)\n",
        "y_i = validate(y_i)\n",
        "input_cols = X_i.drop(columns=[\n",
        "    \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "    \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "    \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"]).columns\n",
        "output_cols = [\"ZWF36_D\"]\n",
        "\n",
        "prepaired = pd.merge(X_i, y_i[[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\", \"ZWF36_D\"]], on=[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"])\n",
        "prepaired.drop(columns=[\n",
        "    \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "    \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "    \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"], inplace=True)\n",
        "\n",
        "\n",
        "train, test = train_test_split(prepaired, test_size=0.2)\n",
        "train = train.reset_index()\n",
        "test = test.reset_index()\n",
        "train = train.drop(columns='index')\n",
        "test = test.drop(columns='index')\n",
        "train['engine_position'] = train['engine_position'].astype('float')\n",
        "test['engine_position']= test['engine_position'].astype('float')\n",
        "train_dataset = DataFrameDataset(train, input_cols, output_cols)\n",
        "test_dataset = DataFrameDataset(test, input_cols, output_cols)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=1)\n",
        "dataloaders[key] = {\"train\": train_dataloader,\n",
        "                    \"train_size\": len(input_cols),\n",
        "                    \"valid\": test_dataloader,\n",
        "                    \"valid_size\": len(output_cols)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGegMP7UyUR8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnDQUXy9WI2s"
      },
      "source": [
        "0) приветствие\n",
        "\n",
        "1) проблематика + ценность\n",
        "\n",
        "2-4) что ценного сделали (решение проекта -- тех. часть, масштабирование)\n",
        "\n",
        "5) демо + QR?\n",
        "\n",
        "6) спасибо за внимание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHidIjNNY_SD"
      },
      "outputs": [],
      "source": [
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self,yhat,y):\n",
        "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKiB6lsFZNtu"
      },
      "outputs": [],
      "source": [
        "test_key = ('CRUISE', 'CF34-8E')\n",
        "input_size = dataloaders[test_key][\"train_size\"]\n",
        "hidden_size1 = 128\n",
        "output_size = dataloaders[test_key][\"valid_size\"]\n",
        "learning_rate = 1000\n",
        "num_epochs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ZLY3fc645I"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(input_size, hidden_size1),\n",
        "    nn.Tanh()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUpG4cW97CZt"
      },
      "outputs": [],
      "source": [
        "criterion = RMSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y6DuOk5gP49",
        "outputId": "16412cdd-b6f1-4f59-fa29-ace43486f80e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train_Loss: 23.9305\n",
            "Epoch 1, Valid_Loss: 23.5957\n",
            "Epoch 2, Train_Loss: 23.9520\n",
            "Epoch 2, Valid_Loss: 23.5957\n",
            "Epoch 3, Train_Loss: 23.9435\n",
            "Epoch 3, Valid_Loss: 23.5957\n",
            "Epoch 4, Train_Loss: 23.9731\n",
            "Epoch 4, Valid_Loss: 23.5957\n",
            "Epoch 5, Train_Loss: 23.9418\n",
            "Epoch 5, Valid_Loss: 23.5957\n",
            "Epoch 6, Train_Loss: 23.9524\n",
            "Epoch 6, Valid_Loss: 23.5957\n",
            "Epoch 7, Train_Loss: 23.9262\n",
            "Epoch 7, Valid_Loss: 23.5957\n",
            "Epoch 8, Train_Loss: 23.9698\n",
            "Epoch 8, Valid_Loss: 23.5957\n",
            "Epoch 9, Train_Loss: 23.9614\n",
            "Epoch 9, Valid_Loss: 23.5957\n",
            "Epoch 10, Train_Loss: 23.9692\n",
            "Epoch 10, Valid_Loss: 23.5957\n",
            "Epoch 11, Train_Loss: 23.9006\n",
            "Epoch 11, Valid_Loss: 23.5957\n",
            "Epoch 12, Train_Loss: 23.9553\n",
            "Epoch 12, Valid_Loss: 23.5957\n",
            "Epoch 13, Train_Loss: 23.9388\n",
            "Epoch 13, Valid_Loss: 23.5957\n",
            "Epoch 14, Train_Loss: 23.9303\n",
            "Epoch 14, Valid_Loss: 23.5957\n",
            "Epoch 15, Train_Loss: 23.9651\n",
            "Epoch 15, Valid_Loss: 23.5957\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for k, dataloader in dataloaders[test_key].items():\n",
        "        epoch_correct = 0\n",
        "        epoch_all = 0\n",
        "\n",
        "\n",
        "        if k != \"train\" and k != \"valid\":\n",
        "            continue\n",
        "        if k == 'train':\n",
        "          train_loss = .0\n",
        "          for x_batch, y_batch in dataloader:\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                outp = model(x_batch)\n",
        "                loss = criterion(outp, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * x_batch.shape[0]\n",
        "          train_loss /= len(dataloader.dataset)\n",
        "          print(f'Epoch {epoch+1}, Train_Loss: {train_loss:.4f}')\n",
        "\n",
        "        elif k == 'valid':\n",
        "          valid_loss = .0\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            for x_batch, y_batch in dataloader:\n",
        "                outp = model(x_batch)\n",
        "                loss = criterion(outp, y_batch )\n",
        "                valid_loss += loss.item() * x_batch.shape[0]\n",
        "            valid_loss /= len(dataloader.dataset)\n",
        "            print(f'Epoch {epoch+1}, Valid_Loss: {valid_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnYFiHcptBBa"
      },
      "source": [
        "Видим, что персептрон плохо подходить для прогнозирования параметра fuel flow"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}