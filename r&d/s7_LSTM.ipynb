{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZT7WFMQVAsI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Dict, Tuple, Set, Callable\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_JMxY6O3fxe",
        "outputId": "fa92beb1-3a9a-40a1-c825-7a0004762898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtEFpFMc4K6J"
      },
      "outputs": [],
      "source": [
        "def grouper(input: Dict[Tuple[str, ...],pd.DataFrame],\n",
        "            keys: Tuple[str, ...]) -> Dict[Tuple[str, ...], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Helper function for recursive grouping dict of pandas DataFrames.\n",
        "    \"\"\"\n",
        "    if not keys:\n",
        "        return input\n",
        "\n",
        "    to_tuple_transformer: Callable = lambda element: element if isinstance(element, tuple) else (element, )\n",
        "\n",
        "    grouped_df: Dict[Tuple[str, ...], pd.DataFrame] = {\n",
        "        (next_key, *to_tuple_transformer(curr_key)): next_df\n",
        "        for curr_key, curr_df in input.items()\n",
        "        for next_key, next_df in curr_df.groupby(keys[-1])\n",
        "    }\n",
        "\n",
        "    return grouper(grouped_df, keys[:-1])\n",
        "\n",
        "\n",
        "def group(input: pd.DataFrame,\n",
        "          keys: Tuple[str, ...] = (\"engine_family\", \"flight_phase\")) -> Dict[Tuple[str, ...], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Mock function for maintenance characteristics prediction.\n",
        "    :param input: input DataFrame of aircraft and engine characteristics.\n",
        "    :param keys: keys that are used for grouping output data.\n",
        "    :return: output groups of DataFrames of predicted maintenance characteristics.\n",
        "    \"\"\"\n",
        "    phase_df: Dict = {k: v for k, v in input.groupby(keys[-1])}\n",
        "\n",
        "    return grouper(phase_df, keys[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoK8KFsi8DZO"
      },
      "outputs": [],
      "source": [
        "def validate(df):\n",
        "    return df.dropna(axis='columns', thresh=20).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTm1xuisVQNG"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('/content/drive/MyDrive/S7/X.csv')\n",
        "y = pd.read_csv('/content/drive/MyDrive/S7/y.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQrY1qijWC1z"
      },
      "outputs": [],
      "source": [
        "splitted_X = group(X, (\"flight_phase\", \"engine_family\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgGFi4kWWKuZ"
      },
      "outputs": [],
      "source": [
        "merged_y = pd.merge(X[[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"]], y, on=[\"engine_id\", \"flight_datetime\", \"flight_phase\"])\n",
        "splitted_y = group(merged_y, (\"flight_phase\", \"engine_family\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAeMlVA4cQ5W"
      },
      "outputs": [],
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df, incols, outcols):\n",
        "        self.df = df\n",
        "        self.incols = incols\n",
        "        self.outcols = outcols\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.df.count()[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if type(index) == tuple:\n",
        "          ind1 = index[0]\n",
        "          ind2 = index[1]\n",
        "          input = torch.tensor(self.df[self.incols].iloc[ind2], dtype=torch.float32)\n",
        "          output = torch.tensor(self.df[self.outcols].iloc[ind2], dtype=torch.float32)\n",
        "          return input[ind1], output[ind1]\n",
        "        else:\n",
        "          input = torch.tensor(self.df[self.incols].iloc[index], dtype=torch.float32)\n",
        "          output = torch.tensor(self.df[self.outcols].iloc[index], dtype=torch.float32)\n",
        "          return input, output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(dataset, lookback):\n",
        "    \"\"\"Transform a time series into a prediction dataset\n",
        "    \n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        lookback: Size of window for prediction\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(dataset)-lookback):\n",
        "        feature = dataset[i : i+lookback, i]\n",
        "        target = dataset[i+1 : i+lookback+1, i]\n",
        "        X.append(feature)\n",
        "        y.append(target)\n",
        "    return torch.tensor(X), torch.tensor(y)"
      ],
      "metadata": {
        "id": "KFRBXk0ikYEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4qp9V5kLZhd"
      },
      "outputs": [],
      "source": [
        "dataloaders = {}\n",
        "for (key, X_i), (_, y_i) in zip(splitted_X.items(), splitted_y.items()):\n",
        "    X_i = validate(X_i)\n",
        "    y_i = validate(y_i)\n",
        "    input_cols = X_i.drop(columns=[\n",
        "        \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "        \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "        \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"]).columns\n",
        "    output_cols = y_i.drop(columns=[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"]).columns\n",
        "\n",
        "    prepaired = pd.merge(X_i, y_i, on=[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"])\n",
        "    prepaired.drop(columns=[\n",
        "        \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "        \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "        \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"], inplace=True)\n",
        "    \n",
        "    train, test = train_test_split(prepaired, test_size=0.2)\n",
        "    train = train.reset_index()\n",
        "    test = test.reset_index()\n",
        "    train = train.drop(columns='index')\n",
        "    test = test.drop(columns='index')\n",
        "    train['engine_position'] = train['engine_position'].astype('float')\n",
        "    test = test['engine_position'].astype('float')\n",
        "    train_dataset = DataFrameDataset(train, input_cols, output_cols)\n",
        "    test_dataset = DataFrameDataset(test, input_cols, output_cols)\n",
        "    train_dataset = create_dataset(train_dataset, 10)\n",
        "    test_dataset = create_dataset(test_dataset, 10)\n",
        "    \n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=1)\n",
        "    dataloaders[key] = {\"train\": train_dataloader,\n",
        "                        \"train_size\": len(input_cols),\n",
        "                        \"valid\": test_dataloader,\n",
        "                        \"valid_size\": len(output_cols)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdCjPQ5Bqx3L"
      },
      "outputs": [],
      "source": [
        "dataloaders = {}\n",
        "(key, X_i), (_, y_i) = next(zip(splitted_X.items(), splitted_y.items()))\n",
        "X_i = validate(X_i)\n",
        "y_i = validate(y_i)\n",
        "input_cols = X_i.drop(columns=[\n",
        "    \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "    \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "    \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"]).columns\n",
        "output_cols = [\"ZWF36_D\"]\n",
        "\n",
        "prepaired = pd.merge(X_i, y_i[[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\", \"ZWF36_D\"]], on=[\"engine_id\", \"flight_datetime\", \"flight_phase\", \"engine_family\"])\n",
        "prepaired.drop(columns=[\n",
        "    \"engine_id\", \"aircraft_id\", \"flight_datetime\",\n",
        "    \"flight_phase\", \"engine_family\", \"engine_type\", \"manufacturer\",\n",
        "    \"aircraft_family\", \"aircraft_type\", \"aircraft_grp\", \"ac_manufacturer\"], inplace=True)\n",
        "\n",
        "\n",
        "train, test = train_test_split(prepaired, test_size=0.2)\n",
        "train = train.reset_index()\n",
        "test = test.reset_index()\n",
        "train = train.drop(columns='index')\n",
        "test = test.drop(columns='index')\n",
        "train['engine_position'] = train['engine_position'].astype('float')\n",
        "test['engine_position']= test['engine_position'].astype('float')\n",
        "train_dataset = DataFrameDataset(train, input_cols, output_cols)\n",
        "test_dataset = DataFrameDataset(test, input_cols, output_cols)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=1)\n",
        "dataloaders[key] = {\"train\": train_dataloader,\n",
        "                    \"train_size\": len(input_cols),\n",
        "                    \"valid\": test_dataloader,\n",
        "                    \"valid_size\": len(output_cols)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGegMP7UyUR8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHidIjNNY_SD"
      },
      "outputs": [],
      "source": [
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self,yhat,y):\n",
        "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKiB6lsFZNtu"
      },
      "outputs": [],
      "source": [
        "test_key = ('CRUISE', 'CF34-8E')\n",
        "input_size = dataloaders[test_key][\"train_size\"]\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 128\n",
        "output_size = dataloaders[test_key][\"valid_size\"]\n",
        "learning_rate = 1000\n",
        "num_epochs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ZLY3fc645I"
      },
      "outputs": [],
      "source": [
        "class AirModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm1 = nn.LSTMCell(input_size, hidden_size1)\n",
        "        self.lstm2 = nn.LSTMCell(hidden_size1, hidden_size2)\n",
        "        self.linear = nn.Linear(hidden_size2, 1)\n",
        "    def forward(self, x):\n",
        "        h_t, c_t = self.lstm1(x) # initial hidden and cell states\n",
        "        h_t2, c_t2 = self.lstm2(h_t) # new hidden and cell states\n",
        "        output = self.linear(h_t2) # output from the last FC layer\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUpG4cW97CZt"
      },
      "outputs": [],
      "source": [
        "model = AirModel()\n",
        "criterion = RMSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y6DuOk5gP49",
        "outputId": "093a6ee3-9aa6-48cd-8e41-a3dd6cae9a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train_Loss: 6888.6151\n",
            "Epoch 1, Valid_Loss: 9218.2325\n",
            "Epoch 2, Train_Loss: 6553.0729\n",
            "Epoch 2, Valid_Loss: 2371.8382\n",
            "Epoch 3, Train_Loss: 6551.7599\n",
            "Epoch 3, Valid_Loss: 4763.4931\n",
            "Epoch 4, Train_Loss: 6542.9852\n",
            "Epoch 4, Valid_Loss: 9865.6993\n",
            "Epoch 5, Train_Loss: 6553.9445\n",
            "Epoch 5, Valid_Loss: 7652.2018\n",
            "Epoch 6, Train_Loss: 6551.0681\n",
            "Epoch 6, Valid_Loss: 3773.0108\n",
            "Epoch 7, Train_Loss: 6543.4293\n",
            "Epoch 7, Valid_Loss: 6020.7443\n",
            "Epoch 8, Train_Loss: 6551.8268\n",
            "Epoch 8, Valid_Loss: 8741.5046\n",
            "Epoch 9, Train_Loss: 6549.8943\n",
            "Epoch 9, Valid_Loss: 8664.2141\n",
            "Epoch 10, Train_Loss: 6550.0229\n",
            "Epoch 10, Valid_Loss: 2866.1391\n",
            "Epoch 11, Train_Loss: 6547.6099\n",
            "Epoch 11, Valid_Loss: 5194.0581\n",
            "Epoch 12, Train_Loss: 6545.5316\n",
            "Epoch 12, Valid_Loss: 9476.8305\n",
            "Epoch 13, Train_Loss: 6552.1875\n",
            "Epoch 13, Valid_Loss: 8000.2696\n",
            "Epoch 14, Train_Loss: 6550.6206\n",
            "Epoch 14, Valid_Loss: 3463.1244\n",
            "Epoch 15, Train_Loss: 6544.5391\n",
            "Epoch 15, Valid_Loss: 5734.3843\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for k, dataloader in dataloaders[test_key].items():\n",
        "        epoch_correct = 0\n",
        "        epoch_all = 0\n",
        "\n",
        "\n",
        "        if k != \"train\" and k != \"valid\":\n",
        "            continue\n",
        "        if k == 'train':\n",
        "          train_loss = .0\n",
        "          for x_batch, y_batch in dataloader:\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                outp = model(x_batch)\n",
        "                loss = criterion(outp, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * x_batch.shape[0]\n",
        "          train_loss /= len(dataloader.dataset)\n",
        "          print(f'Epoch {epoch+1}, Train_Loss: {train_loss:.4f}')\n",
        "\n",
        "        elif k == 'valid':\n",
        "          valid_loss = .0\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            for x_batch, y_batch in dataloader:\n",
        "                outp = model(x_batch)\n",
        "                loss = criterion(outp, y_batch )\n",
        "                valid_loss += loss.item() * x_batch.shape[0]\n",
        "            valid_loss /= len(dataloader.dataset)\n",
        "            print(f'Epoch {epoch+1}, Valid_Loss: {valid_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnYFiHcptBBa"
      },
      "source": [
        "Видим, что персептрон плохо подходить для прогнозирования параметра fuel flow"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
